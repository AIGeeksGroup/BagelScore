#!/usr/bin/env python3
"""
BAGELç›¸ä¼¼æ€§è®¡ç®—å·¥å…·å‡½æ•°
"""

import os
import json
import csv
import numpy as np
from datetime import datetime
from typing import Dict, List, Optional, Union, Any
from PIL import Image
import logging
from pathlib import Path

from .config import BagelSimilarityConfig, DataConfig

logger = logging.getLogger(__name__)


def load_image(image_path: str) -> Image.Image:
    """
    åŠ è½½å›¾åƒæ–‡ä»¶
    
    Args:
        image_path: å›¾åƒæ–‡ä»¶è·¯å¾„
        
    Returns:
        PILå›¾åƒå¯¹è±¡
    """
    try:
        image = Image.open(image_path)
        # è½¬æ¢ä¸ºRGBæ¨¡å¼
        if image.mode != 'RGB':
            image = image.convert('RGB')
        return image
    except Exception as e:
        raise ValueError(f"Failed to load image {image_path}: {e}")


def validate_image(image: Image.Image, data_config: DataConfig) -> bool:
    """
    éªŒè¯å›¾åƒæ˜¯å¦ç¬¦åˆè¦æ±‚
    
    Args:
        image: PILå›¾åƒå¯¹è±¡
        data_config: æ•°æ®é…ç½®
        
    Returns:
        æ˜¯å¦æœ‰æ•ˆ
    """
    if image is None:
        return False
    
    # æ£€æŸ¥å›¾åƒå°ºå¯¸
    width, height = image.size
    if width < data_config.min_image_size or height < data_config.min_image_size:
        logger.warning(f"Image too small: {width}x{height}")
        return False
    
    if width > data_config.max_image_size or height > data_config.max_image_size:
        logger.warning(f"Image too large: {width}x{height}")
        return False
    
    return True


def create_output_directory(config: BagelSimilarityConfig) -> str:
    """
    åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºç›®å½•
    
    Args:
        config: é…ç½®å¯¹è±¡
        
    Returns:
        è¾“å‡ºç›®å½•è·¯å¾„
    """
    if config.output.include_timestamp:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = f"{config.output.output_dir_prefix}_{timestamp}"
    else:
        output_dir = config.output.output_dir_prefix
    
    os.makedirs(output_dir, exist_ok=True)
    logger.info(f"Created output directory: {output_dir}")
    return output_dir


def save_results(results: Dict[str, Any], output_dir: str, image_name: str, config: BagelSimilarityConfig):
    """
    ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    
    Args:
        results: ç»“æœå­—å…¸
        output_dir: è¾“å‡ºç›®å½•
        image_name: å›¾åƒåç§°
        config: é…ç½®å¯¹è±¡
    """
    # ä¿å­˜JSONç»“æœ
    if config.output.save_json:
        json_filename = f"{image_name}_results.json"
        json_path = os.path.join(output_dir, json_filename)
        
        # å¤„ç†numpyæ•°ç»„å’Œtensorçš„åºåˆ—åŒ–
        serializable_results = _make_serializable(results)
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(serializable_results, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Results saved to: {json_path}")
    
    # ä¿å­˜æ‘˜è¦åˆ°æ–‡æœ¬æ–‡ä»¶
    if config.output.save_summary:
        summary_filename = f"{image_name}_summary.txt"
        summary_path = os.path.join(output_dir, summary_filename)
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(f"Image: {image_name}\n")
            f.write(f"Prompt: {results.get('prompt', 'N/A')}\n")
            f.write(f"Calculation Time: {results.get('timestamp', 'N/A')}\n")
            f.write(f"Processing Time: {results.get('calculation_time', 0):.2f} seconds\n\n")
            
            if 'bagel_score_decon' in results and results['bagel_score_decon'] is not None:
                f.write(f"BagelScore_DeCon (Decoding Consistency Score): {results['bagel_score_decon']:.4f}\n")
            
            if 'bagel_score_langimgcon' in results and results['bagel_score_langimgcon'] is not None:
                f.write(f"BagelScore_LangImgCon (Language-Image Consistency Score): {results['bagel_score_langimgcon']:.4f}\n")
            
            if 'language_response' in results and results['language_response']:
                f.write(f"Generated Language Response: {results['language_response']}\n")
            
            if 'error' in results:
                f.write(f"Error: {results['error']}\n")
        
        logger.info(f"Summary saved to: {summary_path}")


def _make_serializable(obj: Any) -> Any:
    """
    å°†å¯¹è±¡è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    
    Args:
        obj: è¾“å…¥å¯¹è±¡
        
    Returns:
        å¯åºåˆ—åŒ–çš„å¯¹è±¡
    """
    if isinstance(obj, dict):
        return {k: _make_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [_make_serializable(item) for item in obj]
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif hasattr(obj, 'item'):  # torch.Tensor
        return obj.item()
    elif hasattr(obj, 'size'):  # PIL.Image
        return f"PIL.Image({obj.size})"
    else:
        return obj


def save_batch_summary(results: List[Dict[str, Any]], output_dir: str, config: BagelSimilarityConfig):
    """
    ä¿å­˜æ‰¹é‡å¤„ç†ç»“æœæ‘˜è¦
    
    Args:
        results: ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        config: é…ç½®å¯¹è±¡
    """
    if not config.output.save_json:
        return
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    successful_results = [r for r in results if 'error' not in r]
    failed_results = [r for r in results if 'error' in r]
    
    summary = {
        'total_images': len(results),
        'successful_images': len(successful_results),
        'failed_images': len(failed_results),
        'timestamp': datetime.now().isoformat(),
        'config': config.to_dict()
    }
    
    if successful_results:
        # è®¡ç®—å¹³å‡åˆ†æ•°
        decon_scores = [r['bagel_score_decon'] for r in successful_results if r.get('bagel_score_decon') is not None]
        langimgcon_scores = [r['bagel_score_langimgcon'] for r in successful_results if r.get('bagel_score_langimgcon') is not None]
        
        if decon_scores:
            summary['decon_stats'] = {
                'mean': np.mean(decon_scores),
                'std': np.std(decon_scores),
                'min': np.min(decon_scores),
                'max': np.max(decon_scores)
            }
        
        if langimgcon_scores:
            summary['langimgcon_stats'] = {
                'mean': np.mean(langimgcon_scores),
                'std': np.std(langimgcon_scores),
                'min': np.min(langimgcon_scores),
                'max': np.max(langimgcon_scores)
            }
        
        # è®¡ç®—æ€»æ—¶é—´
        total_time = sum(r.get('calculation_time', 0) for r in successful_results)
        summary['total_time'] = total_time
        summary['average_time'] = total_time / len(successful_results)
    
    # ä¿å­˜æ‘˜è¦
    summary_path = os.path.join(output_dir, "batch_summary.json")
    with open(summary_path, 'w', encoding='utf-8') as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Batch summary saved to: {summary_path}")
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    detailed_path = os.path.join(output_dir, "all_results.json")
    serializable_results = _make_serializable(results)
    with open(detailed_path, 'w', encoding='utf-8') as f:
        json.dump(serializable_results, f, ensure_ascii=False, indent=2)
    
    logger.info(f"Detailed results saved to: {detailed_path}")


def get_image_files(data_dir: str, supported_formats: List[str]) -> List[str]:
    """
    è·å–æŒ‡å®šç›®å½•ä¸‹çš„å›¾åƒæ–‡ä»¶åˆ—è¡¨
    
    Args:
        data_dir: æ•°æ®ç›®å½•
        supported_formats: æ”¯æŒçš„å›¾åƒæ ¼å¼
        
    Returns:
        å›¾åƒæ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"Data directory does not exist: {data_dir}")
    
    image_files = []
    for file in os.listdir(data_dir):
        if any(file.lower().endswith(ext) for ext in supported_formats):
            image_files.append(os.path.join(data_dir, file))
    
    return sorted(image_files)


def print_results_summary(results: Dict[str, Any], image_name: str = "Unknown"):
    """
    æ‰“å°ç»“æœæ‘˜è¦
    
    Args:
        results: ç»“æœå­—å…¸
        image_name: å›¾åƒåç§°
    """
    print("\n" + "="*60)
    print("BAGEL Similarity Score Calculation Results")
    print("="*60)
    print(f"Test Image: {image_name}")
    
    if 'image_size' in results:
        print(f"Image Size: {results['image_size']}")
    
    if 'prompt' in results:
        print(f"Prompt: {results['prompt']}")
    
    print(f"Calculation Time: {results.get('timestamp', 'N/A')}")
    print(f"Processing Time: {results.get('calculation_time', 0):.2f} seconds")
    print("-" * 60)
    
    if 'bagel_score_decon' in results and results['bagel_score_decon'] is not None:
        print(f"BagelScore_DeCon (Decoding Consistency Score): {results['bagel_score_decon']:.4f}")
    
    if 'bagel_score_langimgcon' in results and results['bagel_score_langimgcon'] is not None:
        print(f"BagelScore_LangImgCon (Language-Image Consistency Score): {results['bagel_score_langimgcon']:.4f}")
    
    print("-" * 60)
    
    if 'language_response' in results and results['language_response']:
        print("Generated Language Response:")
        print(results['language_response'])
        print("-" * 60)
    
    if 'error' in results:
        print(f"Error: {results['error']}")
        print("-" * 60)


def print_batch_summary(results: List[Dict[str, Any]]):
    """
    æ‰“å°æ‰¹é‡å¤„ç†ç»“æœæ‘˜è¦
    
    Args:
        results: ç»“æœåˆ—è¡¨
    """
    successful_results = [r for r in results if 'error' not in r]
    failed_results = [r for r in results if 'error' in r]
    
    print("\n" + "="*60)
    print("Batch Test Results Summary")
    print("="*60)
    print(f"Total Images: {len(results)}")
    print(f"Successfully Processed: {len(successful_results)}")
    print(f"Failed: {len(failed_results)}")
    
    if successful_results:
        total_time = sum(r.get('calculation_time', 0) for r in successful_results)
        print(f"Total Time: {total_time:.2f} seconds")
        print(f"Average Time: {total_time / len(successful_results):.2f} seconds per image")
        print("-" * 60)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        decon_scores = [r['bagel_score_decon'] for r in successful_results if r.get('bagel_score_decon') is not None]
        langimgcon_scores = [r['bagel_score_langimgcon'] for r in successful_results if r.get('bagel_score_langimgcon') is not None]
        
        if decon_scores:
            print(f"BagelScore_DeCon - Average: {np.mean(decon_scores):.4f}, Std: {np.std(decon_scores):.4f}")
            print(f"  Range: [{np.min(decon_scores):.4f}, {np.max(decon_scores):.4f}]")
        
        if langimgcon_scores:
            print(f"BagelScore_LangImgCon - Average: {np.mean(langimgcon_scores):.4f}, Std: {np.std(langimgcon_scores):.4f}")
            print(f"  Range: [{np.min(langimgcon_scores):.4f}, {np.max(langimgcon_scores):.4f}]")
    
    if failed_results:
        print("-" * 60)
        print("Failed Images:")
        for result in failed_results:
            print(f"  - {result.get('image_path', 'Unknown')}: {result.get('error', 'Unknown error')}")


def export_results_to_csv(results: List[Dict[str, Any]], output_path: str) -> None:
    """
    Export batch results to CSV format for analysis
    
    Args:
        results: List of calculation results
        output_path: CSV file path
    """
    try:
        with open(output_path, 'w', newline='', encoding='utf-8') as csvfile:
            # Define CSV headers
            fieldnames = [
                'image_path', 'image_name', 'image_size_width', 'image_size_height',
                'bagel_score_decon', 'bagel_score_langimgcon', 
                'calculation_time', 'timestamp', 'prompt',
                'language_response_length', 'status', 'error'
            ]
            
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
            writer.writeheader()
            
            for result in results:
                # Extract image name from path
                image_name = os.path.basename(result.get('image_path', ''))
                
                # Get image size
                image_size = result.get('image_size', [0, 0])
                width = image_size[0] if len(image_size) > 0 else 0
                height = image_size[1] if len(image_size) > 1 else 0
                
                # Calculate language response length
                lang_response = result.get('language_response', '')
                lang_length = len(lang_response) if lang_response else 0
                
                # Determine status
                status = 'success' if 'error' not in result else 'failed'
                
                # Create CSV row
                csv_row = {
                    'image_path': result.get('image_path', ''),
                    'image_name': image_name,
                    'image_size_width': width,
                    'image_size_height': height,
                    'bagel_score_decon': result.get('bagel_score_decon', ''),
                    'bagel_score_langimgcon': result.get('bagel_score_langimgcon', ''),
                    'calculation_time': result.get('calculation_time', ''),
                    'timestamp': result.get('timestamp', ''),
                    'prompt': result.get('prompt', ''),
                    'language_response_length': lang_length,
                    'status': status,
                    'error': result.get('error', '')
                }
                
                writer.writerow(csv_row)
        
        logger.info(f"CSV results exported to: {output_path}")
        print(f"CSV results exported to: {output_path}")
        
    except Exception as e:
        logger.error(f"Failed to export CSV: {e}")
        print(f"Failed to export CSV: {e}")


def export_results_to_json(results: List[Dict[str, Any]], output_path: str) -> None:
    """
    Export batch results to JSON format for detailed analysis
    
    Args:
        results: List of calculation results
        output_path: JSON file path
    """
    try:
        # Calculate summary statistics
        successful_results = [r for r in results if 'error' not in r]
        failed_results = [r for r in results if 'error' in r]
        
        decon_scores = [r['bagel_score_decon'] for r in successful_results if r.get('bagel_score_decon') is not None]
        langimgcon_scores = [r['bagel_score_langimgcon'] for r in successful_results if r.get('bagel_score_langimgcon') is not None]
        
        summary_stats = {
            'total_images': len(results),
            'successful_count': len(successful_results),
            'failed_count': len(failed_results),
            'bagel_score_decon_stats': {
                'count': len(decon_scores),
                'mean': float(np.mean(decon_scores)) if decon_scores else None,
                'std': float(np.std(decon_scores)) if decon_scores else None,
                'min': float(np.min(decon_scores)) if decon_scores else None,
                'max': float(np.max(decon_scores)) if decon_scores else None,
            },
            'bagel_score_langimgcon_stats': {
                'count': len(langimgcon_scores),
                'mean': float(np.mean(langimgcon_scores)) if langimgcon_scores else None,
                'std': float(np.std(langimgcon_scores)) if langimgcon_scores else None,
                'min': float(np.min(langimgcon_scores)) if langimgcon_scores else None,
                'max': float(np.max(langimgcon_scores)) if langimgcon_scores else None,
            },
            'processing_info': {
                'total_time': sum(r.get('calculation_time', 0) for r in successful_results),
                'average_time': np.mean([r.get('calculation_time', 0) for r in successful_results]) if successful_results else 0,
                'export_timestamp': datetime.now().isoformat()
            }
        }
        
        # Create comprehensive export data
        export_data = {
            'summary': summary_stats,
            'results': results,
            'metadata': {
                'export_version': '1.0',
                'bagel_model': 'BAGEL-7B-MoT',
                'calculation_types': ['BagelScore_DeCon', 'BagelScore_LangImgCon']
            }
        }
        
        with open(output_path, 'w', encoding='utf-8') as jsonfile:
            json.dump(export_data, jsonfile, indent=2, ensure_ascii=False)
        
        logger.info(f"JSON results exported to: {output_path}")
        print(f"JSON results exported to: {output_path}")
        
    except Exception as e:
        logger.error(f"Failed to export JSON: {e}")
        print(f"Failed to export JSON: {e}")


def export_batch_results(results: List[Dict[str, Any]], output_dir: str) -> Dict[str, str]:
    """
    Export batch results to both CSV and JSON formats
    
    Args:
        results: List of calculation results
        output_dir: Output directory path
        
    Returns:
        Dictionary with export file paths
    """
    try:
        # Ensure output directory exists
        os.makedirs(output_dir, exist_ok=True)
        
        # Define export file paths
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        csv_path = os.path.join(output_dir, f"batch_summary_{timestamp}.csv")
        json_path = os.path.join(output_dir, f"all_results_{timestamp}.json")
        
        # Export to both formats
        export_results_to_csv(results, csv_path)
        export_results_to_json(results, json_path)
        
        export_paths = {
            'csv': csv_path,
            'json': json_path
        }
        
        print(f"\nğŸ“Š Batch results exported:")
        print(f"  ğŸ“‹ CSV Summary: {csv_path}")
        print(f"  ğŸ“„ JSON Details: {json_path}")
        
        return export_paths
        
    except Exception as e:
        logger.error(f"Failed to export batch results: {e}")
        print(f"Failed to export batch results: {e}")
        return {}

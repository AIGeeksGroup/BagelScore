#!/usr/bin/env python3
"""
åŒGPUå¹¶è¡ŒBagelScoreè®¡ç®—å™¨
å°†å›¾ç‰‡åˆ†é…åˆ°ä¸¤ä¸ªGPUä¸Šå¹¶è¡Œå¤„ç†ï¼Œæå‡è®¡ç®—æ•ˆç‡
"""

import os
import sys
import json
import subprocess
import time
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import shutil
from datetime import datetime

def get_image_files(data_dir, num_images=100):
    """è·å–æŒ‡å®šæ•°é‡çš„å›¾åƒæ–‡ä»¶"""
    supported_formats = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}
    image_files = []
    
    for file_path in Path(data_dir).iterdir():
        if file_path.suffix.lower() in supported_formats:
            image_files.append(str(file_path))
            if len(image_files) >= num_images:
                break
    
    return image_files[:num_images]

def run_gpu_batch(gpu_id, image_batch, batch_id, output_base_dir):
    """åœ¨æŒ‡å®šGPUä¸Šè¿è¡Œä¸€æ‰¹å›¾åƒçš„è®¡ç®—"""
    print(f"ğŸš€ GPU {gpu_id}: Starting batch {batch_id} with {len(image_batch)} images")
    
    # åˆ›å»ºä¸´æ—¶æ•°æ®ç›®å½•
    temp_dir = f"temp_gpu{gpu_id}_batch{batch_id}"
    os.makedirs(temp_dir, exist_ok=True)
    
    try:
        # å¤åˆ¶å›¾åƒåˆ°ä¸´æ—¶ç›®å½•
        for i, img_path in enumerate(image_batch):
            src = img_path
            dst = os.path.join(temp_dir, f"img_{i:03d}_{Path(img_path).name}")
            shutil.copy2(src, dst)
        
        # è®¾ç½®CUDA_VISIBLE_DEVICESç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = str(gpu_id)
        
        # è¿è¡ŒBagelScoreè®¡ç®—
        cmd = [
            sys.executable, '-m', 'bs_cal.cli',
            '--mode', 'batch',
            '--data-dir', temp_dir,
            '--batch-size', str(len(image_batch))
        ]
        
        print(f"ğŸ’» GPU {gpu_id}: Running command: {' '.join(cmd)}")
        start_time = time.time()
        
        result = subprocess.run(
            cmd,
            env=env,
            capture_output=True,
            text=True,
            cwd=os.getcwd()
        )
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        if result.returncode == 0:
            print(f"âœ… GPU {gpu_id}: Batch {batch_id} completed in {processing_time:.1f}s")
            
            # æŸ¥æ‰¾ç”Ÿæˆçš„ç»“æœç›®å½•
            result_dirs = [d for d in os.listdir('.') if d.startswith('bs_cal/bagel_results_')]
            if result_dirs:
                latest_result_dir = max(result_dirs, key=lambda x: os.path.getctime(x))
                
                # é‡å‘½åç»“æœç›®å½•
                new_result_dir = f"{output_base_dir}_gpu{gpu_id}_batch{batch_id}"
                if os.path.exists(latest_result_dir):
                    shutil.move(latest_result_dir, new_result_dir)
                    print(f"ğŸ“ GPU {gpu_id}: Results moved to {new_result_dir}")
                    return new_result_dir, processing_time, len(image_batch)
        else:
            print(f"âŒ GPU {gpu_id}: Batch {batch_id} failed")
            print(f"Error: {result.stderr}")
            return None, processing_time, 0
            
    except Exception as e:
        print(f"âŒ GPU {gpu_id}: Exception in batch {batch_id}: {e}")
        return None, 0, 0
    finally:
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
            
    return None, 0, 0

def merge_results(result_dirs, output_dir):
    """åˆå¹¶å¤šä¸ªGPUçš„è®¡ç®—ç»“æœ"""
    print(f"ğŸ”— Merging results from {len(result_dirs)} directories...")
    
    all_results = []
    total_stats = {
        'total_images': 0,
        'successful_count': 0,
        'failed_count': 0,
        'total_time': 0
    }
    
    # æ”¶é›†æ‰€æœ‰ç»“æœ
    for result_dir in result_dirs:
        if not result_dir or not os.path.exists(result_dir):
            continue
            
        # æŸ¥æ‰¾JSONç»“æœæ–‡ä»¶
        json_files = [f for f in os.listdir(result_dir) if f.startswith('all_results_') and f.endswith('.json')]
        if json_files:
            json_file = os.path.join(result_dir, json_files[0])
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if 'results' in data:
                        all_results.extend(data['results'])
                        
                    if 'summary' in data:
                        summary = data['summary']
                        total_stats['total_images'] += summary.get('total_images', 0)
                        total_stats['successful_count'] += summary.get('successful_count', 0)
                        total_stats['failed_count'] += summary.get('failed_count', 0)
                        total_stats['total_time'] += summary.get('processing_info', {}).get('total_time', 0)
                        
            except Exception as e:
                print(f"âš ï¸ Failed to read {json_file}: {e}")
    
    # åˆ›å»ºåˆå¹¶åçš„è¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # å¯¼å‡ºåˆå¹¶åçš„ç»“æœ
    if all_results:
        from bs_cal.utils import export_batch_results
        export_paths = export_batch_results(all_results, output_dir)
        
        # åˆ›å»ºæ±‡æ€»æŠ¥å‘Š
        summary_report = f"""
# åŒGPUå¹¶è¡Œå¤„ç†æ±‡æ€»æŠ¥å‘Š

## å¤„ç†ç»Ÿè®¡
- æ€»å›¾ç‰‡æ•°: {total_stats['total_images']}
- æˆåŠŸå¤„ç†: {total_stats['successful_count']}
- å¤±è´¥æ•°é‡: {total_stats['failed_count']}
- æ€»å¤„ç†æ—¶é—´: {total_stats['total_time']:.1f}ç§’
- å¹³å‡æ¯å¼ : {total_stats['total_time']/max(total_stats['successful_count'], 1):.1f}ç§’

## åˆ†æ•°ç»Ÿè®¡
"""
        
        if total_stats['successful_count'] > 0:
            decon_scores = [r.get('bagel_score_decon') for r in all_results if r.get('bagel_score_decon') is not None]
            langimg_scores = [r.get('bagel_score_langimgcon') for r in all_results if r.get('bagel_score_langimgcon') is not None]
            
            if decon_scores:
                import numpy as np
                summary_report += f"""
### BagelScore_DeCon
- å¹³å‡å€¼: {np.mean(decon_scores):.4f}
- æ ‡å‡†å·®: {np.std(decon_scores):.4f}
- èŒƒå›´: [{np.min(decon_scores):.4f}, {np.max(decon_scores):.4f}]
"""
            
            if langimg_scores:
                summary_report += f"""
### BagelScore_LangImgCon  
- å¹³å‡å€¼: {np.mean(langimg_scores):.4f}
- æ ‡å‡†å·®: {np.std(langimg_scores):.4f}
- èŒƒå›´: [{np.min(langimg_scores):.4f}, {np.max(langimg_scores):.4f}]
"""
        
        summary_report += f"""
## è¾“å‡ºæ–‡ä»¶
- CSVç»Ÿè®¡: {export_paths.get('csv', 'N/A')}
- JSONè¯¦æƒ…: {export_paths.get('json', 'N/A')}

ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        report_path = os.path.join(output_dir, 'dual_gpu_summary.md')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        print(f"ğŸ“Š åˆå¹¶å®Œæˆ! ç»“æœä¿å­˜åˆ°: {output_dir}")
        print(f"ğŸ“‹ æ±‡æ€»æŠ¥å‘Š: {report_path}")
        return export_paths
    
    return None

def main():
    """ä¸»å‡½æ•°ï¼šåŒGPUå¹¶è¡Œå¤„ç†"""
    print("ğŸš€ å¯åŠ¨åŒGPUå¹¶è¡ŒBagelScoreè®¡ç®—å™¨")
    
    # é…ç½®å‚æ•°
    DATA_DIR = "data_1000"
    NUM_IMAGES = 100
    OUTPUT_BASE = f"dual_gpu_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    # è·å–å›¾åƒæ–‡ä»¶
    print(f"ğŸ“ ä» {DATA_DIR} è·å– {NUM_IMAGES} å¼ å›¾ç‰‡...")
    image_files = get_image_files(DATA_DIR, NUM_IMAGES)
    
    if len(image_files) < NUM_IMAGES:
        print(f"âš ï¸ åªæ‰¾åˆ° {len(image_files)} å¼ å›¾ç‰‡ï¼Œå°†å¤„ç†æ‰€æœ‰å¯ç”¨å›¾ç‰‡")
        NUM_IMAGES = len(image_files)
    
    # åˆ†é…å›¾ç‰‡åˆ°ä¸¤ä¸ªGPU
    mid_point = NUM_IMAGES // 2
    gpu0_images = image_files[:mid_point]
    gpu1_images = image_files[mid_point:]
    
    print(f"ğŸ”€ å›¾ç‰‡åˆ†é…:")
    print(f"  GPU 0: {len(gpu0_images)} å¼ å›¾ç‰‡")
    print(f"  GPU 1: {len(gpu1_images)} å¼ å›¾ç‰‡")
    
    # å¹¶è¡Œæ‰§è¡Œ
    start_time = time.time()
    result_dirs = []
    processing_times = []
    
    with ThreadPoolExecutor(max_workers=2) as executor:
        # æäº¤ä»»åŠ¡åˆ°ä¸¤ä¸ªGPU
        futures = []
        futures.append(executor.submit(run_gpu_batch, 0, gpu0_images, 1, OUTPUT_BASE))
        futures.append(executor.submit(run_gpu_batch, 1, gpu1_images, 2, OUTPUT_BASE))
        
        # ç­‰å¾…å®Œæˆ
        for future in as_completed(futures):
            result_dir, proc_time, img_count = future.result()
            if result_dir:
                result_dirs.append(result_dir)
            processing_times.append(proc_time)
    
    total_time = time.time() - start_time
    
    print(f"\nâ±ï¸ å¹¶è¡Œå¤„ç†å®Œæˆ! æ€»è€—æ—¶: {total_time:.1f}ç§’")
    print(f"ğŸ“Š å¤„ç†æ•ˆç‡: {NUM_IMAGES/total_time:.2f} å¼ /ç§’")
    
    # åˆå¹¶ç»“æœ
    if result_dirs:
        final_output_dir = f"final_{OUTPUT_BASE}"
        export_paths = merge_results(result_dirs, final_output_dir)
        
        if export_paths:
            print(f"\nğŸ‰ åŒGPUå¹¶è¡Œå¤„ç†å®Œæˆ!")
            print(f"ğŸ“ˆ æœ€ç»ˆç»“æœ:")
            print(f"  ğŸ“Š CSVæ–‡ä»¶: {export_paths.get('csv')}")
            print(f"  ğŸ“„ JSONæ–‡ä»¶: {export_paths.get('json')}")
            print(f"  ğŸ“ ç»“æœç›®å½•: {final_output_dir}")
    else:
        print("âŒ æ²¡æœ‰æˆåŠŸçš„è®¡ç®—ç»“æœ")

if __name__ == "__main__":
    main()
